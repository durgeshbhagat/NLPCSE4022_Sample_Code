{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and Downloading nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nairm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nairm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nairm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "rootdir = 'C:\\\\Users\\\\nairm\\\\.vscode\\\\NLP\\\\NLPCSE4022_Sample_Code\\\\Assignments\\\\Milind Nair\\\\enron1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = ''\n",
    "for directory, subdirectory, filenames in  os.walk(rootdir):\n",
    "    for filename in filenames:\n",
    "        inputfile = os.path.join(directory, filename)\n",
    "        with open(inputfile, \"r\",errors=\"ignore\") as f:\n",
    "            data = f.read().splitlines()\n",
    "            for word in data:\n",
    "                message+=(word+' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1186103"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of words  = {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Unique Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50562"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens  = 50562\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique tokens  = {}\".format(len(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919868"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_stop_words = [word for word in words if word not in stopwords.words('English')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non stop-words  = 919868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Subject',\n",
       " ':',\n",
       " 'christmas',\n",
       " 'tree',\n",
       " 'farm',\n",
       " 'pictures',\n",
       " 'Subject',\n",
       " ':',\n",
       " 'vastar',\n",
       " 'resources']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of non stop-words  = {}\".format(len(non_stop_words)))\n",
    "non_stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject',\n",
       " 'christmas',\n",
       " 'tree',\n",
       " 'farm',\n",
       " 'pictures',\n",
       " 'Subject',\n",
       " 'vastar',\n",
       " 'resources',\n",
       " 'inc',\n",
       " 'gary']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_stop_words = [word for word in non_stop_words if word.isalnum()]\n",
    "non_stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing stop words and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stem_words = []\n",
    "for word in non_stop_words :\n",
    "    stem_words.append(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after stemming = 586501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['subject',\n",
       " 'christma',\n",
       " 'tree',\n",
       " 'farm',\n",
       " 'pictur',\n",
       " 'subject',\n",
       " 'vastar',\n",
       " 'resourc',\n",
       " 'inc',\n",
       " 'gari']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of words after stemming = {}\".format(len(stem_words)))\n",
    "stem_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing stop words and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nairm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "for word in non_stop_words :\n",
    "    lemmatized_words.append(lemmatizer.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subject',\n",
       " 'christmas',\n",
       " 'tree',\n",
       " 'farm',\n",
       " 'picture',\n",
       " 'Subject',\n",
       " 'vastar',\n",
       " 'resource',\n",
       " 'inc',\n",
       " 'gary']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the 100 most frequent and least frequent (rare) word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ect', 13900),\n",
       " ('hou', 7289),\n",
       " ('enron', 6555),\n",
       " ('Subject', 5172),\n",
       " ('2000', 4386),\n",
       " ('com', 3710),\n",
       " ('please', 3200),\n",
       " ('gas', 3034),\n",
       " ('3', 2922),\n",
       " ('subject', 2889),\n",
       " ('deal', 2827),\n",
       " ('1', 2743),\n",
       " ('meter', 2459),\n",
       " ('00', 2404),\n",
       " ('2', 2379),\n",
       " ('cc', 2371),\n",
       " ('pm', 2343),\n",
       " ('hpl', 2318),\n",
       " ('000', 2127),\n",
       " ('10', 2113),\n",
       " ('2001', 2028),\n",
       " ('e', 1976),\n",
       " ('daren', 1901),\n",
       " ('thanks', 1898),\n",
       " ('01', 1794),\n",
       " ('corp', 1776),\n",
       " ('know', 1588),\n",
       " ('0', 1586),\n",
       " ('4', 1577),\n",
       " ('5', 1565),\n",
       " ('need', 1480),\n",
       " ('11', 1440),\n",
       " ('new', 1437),\n",
       " ('may', 1383),\n",
       " ('mmbtu', 1349),\n",
       " ('12', 1345),\n",
       " ('j', 1336),\n",
       " ('forwarded', 1297),\n",
       " ('get', 1276),\n",
       " ('http', 1235),\n",
       " ('03', 1222),\n",
       " ('price', 1206),\n",
       " ('see', 1200),\n",
       " ('company', 1198),\n",
       " ('let', 1160),\n",
       " ('information', 1154),\n",
       " ('farmer', 1141),\n",
       " ('l', 1108),\n",
       " ('attached', 1097),\n",
       " ('7', 1092),\n",
       " ('would', 1078),\n",
       " ('99', 1068),\n",
       " ('6', 1043),\n",
       " ('02', 1040),\n",
       " ('xls', 1020),\n",
       " ('us', 1018),\n",
       " ('day', 1007),\n",
       " ('time', 994),\n",
       " ('message', 966),\n",
       " ('9', 949),\n",
       " ('04', 939),\n",
       " ('30', 935),\n",
       " ('one', 935),\n",
       " ('contract', 920),\n",
       " ('20', 918),\n",
       " ('th', 906),\n",
       " ('volume', 900),\n",
       " ('8', 894),\n",
       " ('mail', 892),\n",
       " ('robert', 886),\n",
       " ('05', 884),\n",
       " ('month', 878),\n",
       " ('sitara', 861),\n",
       " ('09', 860),\n",
       " ('p', 848),\n",
       " ('08', 836),\n",
       " ('email', 833),\n",
       " ('nom', 832),\n",
       " ('texas', 827),\n",
       " ('deals', 808),\n",
       " ('energy', 805),\n",
       " ('volumes', 790),\n",
       " ('questions', 787),\n",
       " ('15', 783),\n",
       " ('sent', 775),\n",
       " ('also', 764),\n",
       " ('www', 756),\n",
       " ('pec', 752),\n",
       " ('change', 735),\n",
       " ('ena', 732),\n",
       " ('bob', 710),\n",
       " ('production', 703),\n",
       " ('x', 697),\n",
       " ('flow', 697),\n",
       " ('call', 694),\n",
       " ('file', 684),\n",
       " ('b', 664),\n",
       " ('like', 661),\n",
       " ('net', 659),\n",
       " ('25', 652)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency = nltk.FreqDist(non_stop_words)\n",
    "frequency.most_common(100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 Least Common Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('moaned', 1),\n",
       " ('unbekannter', 1),\n",
       " ('danke', 1),\n",
       " ('fir', 1),\n",
       " ('liebe', 1),\n",
       " ('supperdoll', 1),\n",
       " ('gefreut', 1),\n",
       " ('meiner', 1),\n",
       " ('mvchte', 1),\n",
       " ('erzdhlen', 1),\n",
       " ('hei', 1),\n",
       " ('jahre', 1),\n",
       " ('ziemlich', 1),\n",
       " ('lustiger', 1),\n",
       " ('leider', 1),\n",
       " ('schichtern', 1),\n",
       " ('hobbys', 1),\n",
       " ('sind', 1),\n",
       " ('meinen', 1),\n",
       " ('inlinern', 1),\n",
       " ('durch', 1),\n",
       " ('botanik', 1),\n",
       " ('sausen', 1),\n",
       " ('dazu', 1),\n",
       " ('hund', 1),\n",
       " ('naja', 1),\n",
       " ('sonst', 1),\n",
       " ('leben', 1),\n",
       " ('spass', 1),\n",
       " ('macht', 1),\n",
       " ('recht', 1),\n",
       " ('spontan', 1),\n",
       " ('lege', 1),\n",
       " ('foto', 1),\n",
       " ('vieleicht', 1),\n",
       " ('gar', 1),\n",
       " ('dein', 1),\n",
       " ('schreibe', 1),\n",
       " ('ganze', 1),\n",
       " ('lebensgeschichte', 1),\n",
       " ('umsonst', 1),\n",
       " ('sollte', 1),\n",
       " ('doch', 1),\n",
       " ('zusagen', 1),\n",
       " ('gefallen', 1),\n",
       " ('privaten', 1),\n",
       " ('erreichen', 1),\n",
       " ('sofort', 1),\n",
       " ('sprechen', 1),\n",
       " ('schreiben', 1),\n",
       " ('abissno', 1),\n",
       " ('wirde', 1),\n",
       " ('freuen', 1),\n",
       " ('wieder', 1),\n",
       " ('hvren', 1),\n",
       " ('erstmal', 1),\n",
       " ('schluss', 1),\n",
       " ('teste', 1),\n",
       " ('spontanitdt', 1),\n",
       " ('6567', 1),\n",
       " ('1758', 1),\n",
       " ('prunespurchasers', 1),\n",
       " ('bebut', 1),\n",
       " ('finesse', 1),\n",
       " ('orand', 1),\n",
       " ('minerva', 1),\n",
       " ('seebut', 1),\n",
       " ('maynot', 1),\n",
       " ('cohn', 1),\n",
       " ('utensil', 1),\n",
       " ('itit', 1),\n",
       " ('precipice', 1),\n",
       " ('infield', 1),\n",
       " ('seeit', 1),\n",
       " ('sleepwalk', 1),\n",
       " ('onbe', 1),\n",
       " ('irradiate', 1),\n",
       " ('referee', 1),\n",
       " ('andor', 1),\n",
       " ('gable', 1),\n",
       " ('credenza', 1),\n",
       " ('cleeqing', 1),\n",
       " ('georgette', 1),\n",
       " ('acourtesy', 1),\n",
       " ('increa', 1),\n",
       " ('stimulated', 1),\n",
       " ('getofflist', 1),\n",
       " ('eden', 1),\n",
       " ('astral', 1),\n",
       " ('turnabout', 1),\n",
       " ('troublesome', 1),\n",
       " ('levis', 1),\n",
       " ('jobop', 1),\n",
       " ('incomeop', 1),\n",
       " ('5887', 1),\n",
       " ('multibithewitt', 1),\n",
       " ('everythingbusiness', 1),\n",
       " ('rndlen', 1),\n",
       " ('bodyhtml', 1),\n",
       " ('bbbbb', 1)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency.most_common()[-100:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c50134b391260745b2fe6a2c1c8b3a0342d42548246a7fa04144079639e446b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
